
# ğŸ“‰ Principal Component Analysis (PCA)

## ğŸ”· Brief Primer & History
Principal Component Analysis (PCA) is a **statistical dimensionality reduction technique** that transforms a dataset with possibly correlated features into a new feature space consisting of **linearly uncorrelated variables** called **Principal Components (PCs)**.

- First component captures **maximum variance**
- Each subsequent component captures maximum remaining variance
- Components are **orthogonal**
- Sensitive to **feature scaling**

Invented by **Karl Pearson (1901)** and later formalized by **Harold Hotelling (1930s)**.

---

## ğŸ§® Mathematical Foundation

Assume a dataset matrix:

- \( \mathbf{X} \) : \( n \times p \) matrix  
- \( n \) = samples  
- \( p \) = features  
- Data is mean-centered

### âœ… Objective
Find a projection that maximizes variance:

\(\max \ \mathrm{Var}(\mathbf{Xw}) \)

subject to

\(\|\mathbf{w}\| = 1\)

---

## âœ” Step 1 â€” Covariance Matrix
Compute covariance matrix:

$$
\mathbf{C} = \frac{1}{n-1}\mathbf{X}^T \mathbf{X}
$$

---

## âœ” Step 2 â€” Eigen Decomposition
Solve for eigenvalues \( \lambda \) and eigenvectors \( \mathbf{w} \):

$$
\mathbf{Cw} = \lambda \mathbf{w}
$$

Where:

- **Eigenvectors** â†’ Principal directions (Principal Components)
- **Eigenvalues** â†’ Variance captured by each PC

---

## âœ” Step 3 â€” Order Components
Eigenvalues ranked:

$$
\lambda_1 \ge \lambda_2 \ge \lambda_3 \dots \ge \lambda_p
$$

Largest eigenvalue â†’ Highest variance direction.

---

## âœ” Step 4 â€” Project Data
Projection of sample \( i \) on component \( k \):

$$
t_k^{(i)} = \mathbf{x}^{(i)} \cdot \mathbf{w}_k
$$

Matrix projection form:

$$
\mathbf{T} = \mathbf{XW}
$$

- \( \mathbf{T} \) â†’ Transformed PCA space
- \( \mathbf{W} \) â†’ Matrix of eigenvectors

---

## ğŸ“Š Variance Explained
Explained Variance Ratio:

$$
\text{EVR}_k = \frac{\lambda_k}{\sum_{i=1}^{p}\lambda_i}
$$

Cumulative:

$$
\text{Cumulative EVR} = \sum_{k=1}^{m} \text{EVR}_k
$$

Used to decide how many PCs to retain.

---

## ğŸ§  Intuition
PCA rotates the coordinate system so that:

- Axisâ€‘1 aligns with maximum variance direction
- Axisâ€‘2 aligns with next maximum orthogonal variance
- Noise + redundancy reduce

Think of PCA as:

> â€œFinding better coordinate axes for your dataâ€

---

## ğŸ¢ Business Value
PCA is widely used in:
- Customer segmentation
- Financial risk analysis
- Image compression
- Noise reduction
- Feature engineering
- Visualization of highâ€‘dimensional data

---

## âš ï¸ When PCA May Not Work Well
- Nonâ€‘linear structure (use tâ€‘SNE or UMAP)
- Highly imbalanced variance
- Not scaled data
- When interpretability of raw features is critical

---

## ğŸ›  Tech Stack Aligned
- NumPy
- Pandas
- Scikitâ€‘learn
- Matplotlib / Seaborn

---

## ğŸ‘¤ Author
**Vinay Sangam**  
_Data & AI Engineer_
