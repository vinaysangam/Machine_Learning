# ğŸ§® Principal Component Analysis (PCA) â€” Professional Learning & Reference Guide

## ğŸ“Œ Overview
Principal Component Analysis (PCA) is a **linear dimensionality reduction** technique used to transform highâ€‘dimensional datasets into a smaller set of meaningful features (Principal Components), while retaining as much variance (information) as possible.

It helps in:
- Reducing dimensionality
- Removing multicollinearity
- Noise reduction
- Visualization of complex data
- Improving downstream ML performance

---

## ğŸ§¾ Brief Primer & History
PCA was introduced in **1901 by Karl Pearson** as an analogue of the principal axes theorem in mechanics and later formalized and extended by **Harold Hotelling in the 1930s**.

It uses an **orthogonal transformation** to convert correlated variables into **uncorrelated principal components**.  
These components are ordered such that:
- 1st component captures the highest variance
- 2nd captures the next highest variance under orthogonality constraint
- and so onâ€¦

PCA is **sensitive to feature scaling**, so normalization / standardization is critical.

---

## ğŸ§  Mathematical Foundation

Consider a dataset matrix:

\mathbf{X} âˆˆ â„^{nÃ—p}

Where:
- n = number of observations
- p = number of features
- X is assumed to be centered (mean of each column is zero)

### Step 1 â€” Compute Covariance Matrix
\mathbf{C} = \frac{1}{n-1} \mathbf{X}^T \mathbf{X}

### Step 2 â€” Eigen Decomposition
Find eigenvalues (Î») and eigenvectors (w):

\mathbf{Cw} = \lambda \mathbf{w}

- Eigenvectors â†’ Principal directions (principal components)
- Eigenvalues â†’ Variance explained by each component

### Step 3 â€” Ordering Components
Components are ranked:

Î»â‚ â‰¥ Î»â‚‚ â‰¥ Î»â‚ƒ â€¦ â‰¥ Î»â‚š

### Step 4 â€” Projection
Principal Component Scores:

tâ‚–(i) = \mathbf{x}_{(i)} \cdot \mathbf{w}_{(k)}

Matrix form:

\mathbf{T} = \mathbf{XW}

Where:
- W = matrix of eigenvectors
- T = transformed dataset in PCA space

### Maximization Objective
PCA maximizes projected variance:

\mathbf{w}_{(1)} = arg max_{||w||=1} (||Xw||Â²)

Subject to:
- orthogonality constraints
- unit length eigenvectors

### Successive Component Extraction
After extracting kâˆ’1 components:

XÌ‚â‚– = X âˆ’ Î£ (X wâ‚› wâ‚›áµ€)

Next component:

\mathbf{w}_{(k)} = arg max \frac{\mathbf{w}^T \mathbf{\hat{X}}^T \mathbf{\hat{X}} \mathbf{w}}{\mathbf{w}^T \mathbf{w}}

In modern practice:
ğŸ“Œ PCA is computed using **Singular Value Decomposition (SVD)** for numerical stability.

---

## ğŸ“‰ Variance Explained
Explained Variance Ratio:

EVRâ‚– = Î»â‚– / Î£ Î»

Cumulative Variance tells how many components we need:

Î£ EVRâ‚– â‰¥ Threshold (e.g., 90â€“95%)

---

## ğŸ” Where PCA is Useful
- Customer Segmentation
- Finance Risk Modeling
- Gene Expression Analysis
- Image Compression
- Noise Filtering
- Recommendation Systems
- Preprocessing for ML Models

---

## âš ï¸ Limitations
- Assumes linear relationships
- Sensitive to feature scaling
- Components may be hard to interpret
- Poor performance on highly nonâ€‘linear datasets â†’ tâ€‘SNE / UMAP preferred

---

## ğŸ› ï¸ Tech Used in Notebook
- Python
- NumPy
- Pandas
- Scikitâ€‘learn
- Matplotlib / Seaborn

---

## ğŸ‘¤ Author
**Vinay Sangam**  
_Data & AI Engineer_

---
â­ Explore more Machine Learning work across the repository!
